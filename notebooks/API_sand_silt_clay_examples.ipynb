{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following in the command line, before proceeding to the examples\n",
    "> pip install quick_pp\n",
    "\n",
    "> quick_pp api-server\n",
    "\n",
    "View the API documentation at localhost:8888/docs or localhost:8888/redoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Setup working directory to the project root.\n",
    "import os\n",
    "cwd = os.getcwd()\n",
    "project_name = 'quick_pp'\n",
    "os.chdir(fr'{cwd.split(project_name)[0] + project_name}')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "import requests\n",
    "\n",
    "import quick_pp.las_handler as las\n",
    "from quick_pp.qaqc import badhole_flagging, handle_outer_limit, neu_den_xplot_hc_correction\n",
    "from quick_pp.plotter.plotter import plotly_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_api_request(df_dict: dict,\n",
    "                     endpoint: str,\n",
    "                     session: requests.Session,\n",
    "                     files: list = [],\n",
    "                     verify: bool = False) -> list:\n",
    "    \"\"\"\"\"\n",
    "    This method sends a prediction request to the FastAPI Swagger UI. It uses the provided DataFrame\n",
    "    dictionary, field, use case, method, and model to make the request. It also uses the provided session\n",
    "    and verify flag.\n",
    "\n",
    "    Args:\n",
    "        df_dict (dict): The DataFrame dictionary to use for the prediction request.\n",
    "        field (str): The field to use for the prediction request.\n",
    "        use_case (str): The use case to use for the prediction request.\n",
    "        method (str): The method to use for the prediction request.\n",
    "        model (str): The model to use for the prediction request.\n",
    "        session (requests.Session, optional): The session to use for the prediction request.\n",
    "        If not provided, a new session will be created. Defaults to None.\n",
    "        verify (bool, optional): The verify flag to use for the prediction request.\n",
    "        If set to False, the SSL certificate will not be verified. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        list: The prediction result if the request is successful.\n",
    "\n",
    "    Raises:\n",
    "        AssertionError: If the first key in the DataFrame dictionary is not \"data\".\n",
    "    \"\"\"\n",
    "    assert \"data\" in df_dict.keys()\n",
    "\n",
    "    # Define model API server to the FastAPI Swagger UI.\n",
    "    model_server = {\n",
    "        \"local\": \"http://localhost:8888\",\n",
    "    }\n",
    "\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"accept\": \"application/json\",\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Create url to access the model API.\n",
    "        url = f\"{model_server['local']}/quick_pp/{endpoint}\"\n",
    "        print(f\"Requesting API to {url}\")\n",
    "\n",
    "        # Get the response from the API.\n",
    "        response = session.post(url=url,\n",
    "                                json=df_dict,\n",
    "                                # headers=headers,\n",
    "                                files=files,\n",
    "                                verify=verify)\n",
    "        status = response.status_code\n",
    "\n",
    "        if status == 200:\n",
    "            return response.json()\n",
    "        else:\n",
    "            print(f\"[make_api_request] Error | {response.text} \")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[make_api_request] Error | {e} \")\n",
    "\n",
    "\n",
    "session = requests.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the prediction request via API.\n",
    "files = [\n",
    "    ('files', open(r'data\\01_raw\\36_7-3.las', 'rb'))\n",
    "]\n",
    "api_data_response = make_api_request(df_dict={'data': []}, endpoint='las_handler', files=files, session=session)\n",
    "print(f\"API response: {api_data_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(fr\"../{api_data_response['file_paths'][0]}\")\n",
    "\n",
    "df['RT'] = df['RDEP']  # Use RDEP instead of RT\n",
    "\n",
    "# Mask outside threshold\n",
    "df = handle_outer_limit(df, True)\n",
    "\n",
    "# Flag bad hole\n",
    "df = badhole_flagging(df) if 'CALI' in df.columns else df\n",
    "# Prepare the data for the prediction request.\n",
    "df.interpolate(inplace=True)  # Interpolate null values\n",
    "df.dropna(inplace=True)  # Drop remaining null values\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lithology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {\n",
    "    \"dry_sand_point\": [\n",
    "        -0.02,\n",
    "        2.65\n",
    "    ],\n",
    "    \"dry_silt_point\": [\n",
    "        0,\n",
    "        2.68\n",
    "    ],\n",
    "    \"dry_clay_point\": [\n",
    "        0,\n",
    "        2.7\n",
    "    ],\n",
    "    \"fluid_point\": [\n",
    "        1,\n",
    "        1\n",
    "    ],\n",
    "    \"wet_clay_point\": [\n",
    "        0,\n",
    "        0\n",
    "    ],\n",
    "    \"method\": \"ssc\",\n",
    "    \"silt_line_angle\": 117,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = pd.DataFrame()\n",
    "df_copy[['nphi', 'rhob']] = df[['NPHI', 'RHOB']]\n",
    "df_dict = {\"data\": df_copy.to_dict(orient=\"records\")}  # Convert data_df to dictionary\n",
    "df_dict.update(data_dict)\n",
    "\n",
    "# Make the prediction request via API.\n",
    "api_data_response = make_api_request(df_dict=df_dict, endpoint='lithology/ssc', session=session)\n",
    "df_ssc = df.copy()\n",
    "df_ssc[['VSAND', 'VSILT', 'VCLAY']] = pd.DataFrame(api_data_response)[['VSAND', 'VSILT', 'VCLAY']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plotly_log(df_ssc)\n",
    "fig.show()\n",
    "# fig.write_html('plot.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hydrocarbon Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = pd.DataFrame()\n",
    "df_copy[['nphi', 'rhob', 'gr']] = df[['NPHI', 'RHOB', 'GR']]\n",
    "df_dict = {\"data\": df_copy.to_dict(orient=\"records\")}  # Convert data_df to dictionary\n",
    "df_dict.update(data_dict)\n",
    "df_dict.update({'dry_clay_point': (0.33, 2.7), 'corr_angle': 50})\n",
    "\n",
    "# Make the prediction request via API.\n",
    "api_data_response = make_api_request(df_dict=df_dict, endpoint='lithology/hc_corr', session=session)\n",
    "df_corr = df.copy()\n",
    "df_corr[['VSAND', 'VSILT', 'VCLAY']] = pd.DataFrame(api_data_response)[['VSAND', 'VSILT', 'VCLAY']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Porosity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_dict = pd.DataFrame()\n",
    "# df_dict[['nphi', 'rhob', 'gr']] = df[['NPHI', 'RHOB', 'GR']]\n",
    "# df_dict = {\"data\": df_dict.to_dict(orient=\"records\")}  # Convert data_df to dictionary\n",
    "# df_dict.update(data_dict)\n",
    "# df_dict.update({'dry_clay_point': (0.33, 2.7), 'corr_angle': 50})\n",
    "\n",
    "# Make the prediction request via API.\n",
    "api_data_response = make_api_request(df_dict=df_dict, endpoint='porosity/den', session=session)\n",
    "df_corr['PHID'] = pd.DataFrame(api_data_response)['PHID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the prediction request via API.\n",
    "df_dict.update({'dry_silt_point': (0.15, 2.68)})\n",
    "api_data_response = make_api_request(df_dict=df_dict, endpoint='porosity/neu_den', session=session)\n",
    "df_corr['PHIT'] = pd.DataFrame(api_data_response)['PHIT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr.plot(x='DEPTH', y=['PHIT', 'PHID'], figsize=(20, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Water Saturation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate the temperature gradient\n",
    "\n",
    "df_copy = pd.DataFrame()\n",
    "df_copy[['tvdss']] = df[['DEPTH']]\n",
    "df_dict = {\"data\": df_copy.to_dict(orient=\"records\")}  # Convert data_df to dictionary\n",
    "\n",
    "# Make the prediction request via API.\n",
    "api_data_response = make_api_request(df_dict=df_dict, endpoint='saturation/temp_grad', session=session)\n",
    "df_corr['TEMP_GRAD'] = pd.DataFrame(api_data_response)['TEMP_GRAD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate the formation water resistivity\n",
    "\n",
    "df_copy = pd.DataFrame()\n",
    "df_copy[['temp_grad']] = df_corr[['TEMP_GRAD']].dropna()\n",
    "df_dict = {\"data\": df_copy.to_dict(orient=\"records\")}  # Convert data_df to dictionary\n",
    "df_dict.update({'water_salinity': 30000})\n",
    "\n",
    "# Make the prediction request via API.\n",
    "api_data_response = make_api_request(df_dict=df_dict, endpoint='saturation/rw', session=session)\n",
    "df_corr['RW'] = pd.DataFrame(api_data_response)['RW']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate archie water saturation\n",
    "\n",
    "df_copy = pd.DataFrame()\n",
    "df_copy[['rt', 'rw', 'phit']] = df_corr[['RT', 'RW', 'PHIT']].dropna()\n",
    "df_dict = {\"data\": df_copy.to_dict(orient=\"records\")}  # Convert data_df to dictionary\n",
    "\n",
    "# Make the prediction request via API.\n",
    "api_data_response = make_api_request(df_dict=df_dict, endpoint='saturation/archie', session=session)\n",
    "df_corr['SWT_A'] = pd.DataFrame(api_data_response)['SWT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate the conductivity factor, b\n",
    "\n",
    "df_dict = pd.DataFrame()\n",
    "df_dict[['temp_grad', 'rw']] = df_corr[['TEMP_GRAD', 'RW']].dropna()\n",
    "df_dict = {\"data\": df_dict.to_dict(orient=\"records\")}  # Convert data_df to dictionary\n",
    "\n",
    "# Make the prediction request via API.\n",
    "api_data_response = make_api_request(df_dict=df_dict, endpoint='saturation/b_waxman_smits', session=session)\n",
    "df_corr['B'] = pd.DataFrame(api_data_response)['B']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate the cation exchange capacity, qv\n",
    "\n",
    "df_dict = pd.DataFrame()\n",
    "df_dict[['vcld', 'phit']] = df_corr[['VCLAY', 'PHIT']].dropna()\n",
    "df_dict = {\"data\": df_dict.to_dict(orient=\"records\")}  # Convert data_df to dictionary\n",
    "df_dict.update({'rho_clay': 2.65, 'cec_clay': 0.062})\n",
    "\n",
    "# Make the prediction request via API.\n",
    "api_data_response = make_api_request(df_dict=df_dict, endpoint='saturation/estimate_qv', session=session)\n",
    "df_corr['QV'] = pd.DataFrame(api_data_response)['QV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate waxman-smits water saturation\n",
    "df_corr['M'] = 2\n",
    "df_dict = pd.DataFrame()\n",
    "df_dict[['rt', 'rw', 'phit', 'b', 'qv', 'm']] = df_corr[['RT', 'RW', 'PHIT', 'B', 'QV', 'M']].dropna()\n",
    "df_dict = {\"data\": df_dict.to_dict(orient=\"records\")}  # Convert data_df to dictionary\n",
    "\n",
    "# Make the prediction request via API.\n",
    "api_data_response = make_api_request(df_dict=df_dict, endpoint='saturation/waxman_smits', session=session)\n",
    "df_corr['SWT'] = pd.DataFrame(api_data_response)['SWT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr.plot(x='DEPTH', y=['SWT', 'SWT_A'], figsize=(20, 5), ylim=(0, 1.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Permeability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate Choo's permeability\n",
    "\n",
    "df_dict = pd.DataFrame()\n",
    "df_dict[['vcld', 'vsilt', 'phit']] = df_corr[['VCLAY', 'VSILT', 'PHIT']].dropna()\n",
    "df_dict = {\"data\": df_dict.to_dict(orient=\"records\")}  # Convert data_df to dictionary\n",
    "# df_dict.update(data_dict)\n",
    "\n",
    "# Make the prediction request via API.\n",
    "api_data_response = make_api_request(df_dict=df_dict, endpoint='permeability/choo', session=session)\n",
    "df_corr['PERM'] = pd.DataFrame(api_data_response)['PERM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate other's permeability\n",
    "constant = df_corr['VCLAY']**1.75\n",
    "df_corr['SWIRR'] = constant / df_corr['PHIT']\n",
    "df_dict = pd.DataFrame()\n",
    "df_dict[['phit', 'swirr']] = df_corr[['PHIT', 'SWIRR']].dropna()\n",
    "df_dict = {\"data\": df_dict.to_dict(orient=\"records\")}  # Convert data_df to dictionary\n",
    "# df_dict.update(data_dict)\n",
    "\n",
    "# Make the prediction request via API.\n",
    "api_data_response = make_api_request(df_dict=df_dict, endpoint='permeability/timur', session=session)\n",
    "df_corr['PERM_T'] = pd.DataFrame(api_data_response)['PERM']\n",
    "\n",
    "# Make the prediction request via API.\n",
    "api_data_response = make_api_request(df_dict=df_dict, endpoint='permeability/tixier', session=session)\n",
    "df_corr['PERM_TX'] = pd.DataFrame(api_data_response)['PERM']\n",
    "\n",
    "# Make the prediction request via API.\n",
    "api_data_response = make_api_request(df_dict=df_dict, endpoint='permeability/coates', session=session)\n",
    "df_corr['PERM_C'] = pd.DataFrame(api_data_response)['PERM']\n",
    "\n",
    "# Make the prediction request via API.\n",
    "api_data_response = make_api_request(df_dict=df_dict, endpoint='permeability/kozeny_carman', session=session)\n",
    "df_corr['PERM_KC'] = pd.DataFrame(api_data_response)['PERM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr.plot(x='DEPTH', y=['PERM', 'PERM_KC', 'PERM_T', 'PERM_C', 'PERM_TX'], figsize=(20, 5), logy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rock Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std = df_corr['VCLAY'].describe()['std']\n",
    "standard_q = [0.2, 0.4, 0.6]\n",
    "proportion = [pct - std for pct in standard_q]\n",
    "proportion = standard_q if any([p < 0.15 for p in proportion]) else proportion\n",
    "q_dict = df_corr['VCLAY'].quantile(proportion).to_dict()\n",
    "q_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr['ROCK_FLAG'] = np.where(df_corr['VCLAY'] < list(q_dict.values())[0], 1,\n",
    "                               np.where(df_corr['VCLAY'] < list(q_dict.values())[1], 2,\n",
    "                                        np.where(df_corr['VCLAY'] < list(q_dict.values())[2], 3, 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reservoir Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate reservoir summary\n",
    "df_corr['ZONES'] = 'ALL'\n",
    "df_copy = pd.DataFrame()\n",
    "df_copy[['depth', 'vcld', 'phit', 'swt', 'perm', 'zones']] = df_corr[\n",
    "    ['DEPTH', 'VCLAY', 'PHIT', 'SWT', 'PERM', 'ZONES']].dropna()\n",
    "df_dict = {\"data\": df_copy.to_dict(orient=\"records\")}  # Convert data_df to dictionary\n",
    "# df_dict.update(data_dict)\n",
    "df_dict.update({'cut_offs': dict(VSHALE=0.4, PHIT=.05, SWT=.8)})\n",
    "\n",
    "# Make the prediction request via API.\n",
    "api_data_response = make_api_request(df_dict=df_dict, endpoint='ressum', session=session)\n",
    "ressum_df = pd.DataFrame(api_data_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ressum_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plotly_log(df_corr)\n",
    "fig.show(config={'scrollZoom': True, 'displayModeBar': True})\n",
    "# fig.write_html('plot.html')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
