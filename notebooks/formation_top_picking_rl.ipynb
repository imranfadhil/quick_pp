{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95261918",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47358ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formation Top Picking with Reinforcement Learning\n",
    "# Run `pip install \"gymnasium[classic-control]\"` for this example.\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# FORMATION TOP PICKING ENVIRONMENT\n",
    "# ============================================================================\n",
    "class FormationTopPickingEnv:\n",
    "    \"\"\"Custom environment for formation top picking using well log data.\"\"\"\n",
    "\n",
    "    def __init__(self, well_data: pd.DataFrame, max_depth_window: int = 50,\n",
    "                 reward_weights: Dict[str, float] = None):\n",
    "        \"\"\"\n",
    "        Initialize the formation top picking environment.\n",
    "\n",
    "        Args:\n",
    "            well_data: DataFrame with columns ['DEPTH', 'GR', 'NPHI', 'RHOB', 'RT', 'VSHALE', 'PHIT']\n",
    "            max_depth_window: Number of depth points to consider in each step\n",
    "            reward_weights: Weights for different reward components\n",
    "        \"\"\"\n",
    "        self.well_data = well_data.copy()\n",
    "        self.max_depth_window = max_depth_window\n",
    "        self.current_depth_idx = 0\n",
    "        self.picked_tops = []\n",
    "        self.episode_reward = 0\n",
    "        self.step_count = 0\n",
    "        self.max_steps = len(well_data) // 10  # Limit episode length\n",
    "\n",
    "        # Normalize log data\n",
    "        self._normalize_logs(skip=True)\n",
    "\n",
    "        # Reward weights\n",
    "        self.reward_weights = reward_weights or {\n",
    "            'lithology_change': 1.0,\n",
    "            'depth_spacing': 0.5,\n",
    "            'log_response': 0.3,\n",
    "            'geological_consistency': 0.2\n",
    "        }\n",
    "\n",
    "        # Action space: 0 = no pick, 1 = pick formation top\n",
    "        self.action_space = gym.spaces.Discrete(2)\n",
    "\n",
    "        # Observation space: normalized log values + context\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=-np.inf,\n",
    "            high=np.inf,\n",
    "            shape=(self.max_depth_window * 6 + 4,),  # 6 logs * window + 4 context features\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "    def _normalize_logs(self, skip=False):\n",
    "        \"\"\"Normalize log data for better learning.\"\"\"\n",
    "        log_columns = ['GR', 'NPHI', 'RHOB', 'RT', 'VSHALE', 'PHIT']\n",
    "\n",
    "        for col in log_columns:\n",
    "            if col in self.well_data.columns and not skip:\n",
    "                # Robust normalization using median and IQR\n",
    "                median_val = self.well_data[col].median()\n",
    "                q75, q25 = self.well_data[col].quantile([0.75, 0.25])\n",
    "                iqr = q75 - q25\n",
    "\n",
    "                if iqr > 0:\n",
    "                    self.well_data[f'{col}_NORM'] = (self.well_data[col] - median_val) / iqr\n",
    "                else:\n",
    "                    self.well_data[f'{col}_NORM'] = 0\n",
    "            elif col in self.well_data.columns and skip:\n",
    "                self.well_data[f'{col}_NORM'] = self.well_data[col]\n",
    "            else:\n",
    "                # Fill with zeros if column doesn't exist\n",
    "                self.well_data[f'{col}_NORM'] = 0\n",
    "\n",
    "    def reset(self, seed=None):\n",
    "        \"\"\"Reset the environment for a new episode.\"\"\"\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "\n",
    "        self.current_depth_idx = 0\n",
    "        self.picked_tops = []\n",
    "        self.episode_reward = 0\n",
    "        self.step_count = 0\n",
    "\n",
    "        return self._get_observation(), {}\n",
    "\n",
    "    def _get_observation(self) -> np.ndarray:\n",
    "        \"\"\"Get current observation including log data and context.\"\"\"\n",
    "        # Get log data window\n",
    "        start_idx = max(0, self.current_depth_idx - self.max_depth_window // 2)\n",
    "        end_idx = min(len(self.well_data), self.current_depth_idx + self.max_depth_window // 2)\n",
    "\n",
    "        window_data = self.well_data.iloc[start_idx:end_idx]\n",
    "\n",
    "        # Extract normalized log values\n",
    "        log_features = []\n",
    "        for col in ['GR_NORM', 'NPHI_NORM', 'RHOB_NORM', 'RT_NORM', 'VSHALE_NORM', 'PHIT_NORM']:\n",
    "            log_features.extend(window_data[col].fillna(0).values)\n",
    "\n",
    "        # Pad if window is smaller than max_depth_window\n",
    "        while len(log_features) < self.max_depth_window * 6:\n",
    "            log_features.extend([0] * 6)\n",
    "\n",
    "        # Add context features\n",
    "        context_features = [\n",
    "            self.current_depth_idx / len(self.well_data),  # Depth progress\n",
    "            len(self.picked_tops) / 20,  # Number of picks (normalized)\n",
    "            self.step_count / self.max_steps,  # Episode progress\n",
    "            self.episode_reward / 100  # Current reward (normalized)\n",
    "        ]\n",
    "\n",
    "        return np.array(log_features + context_features, dtype=np.float32)\n",
    "\n",
    "    def step(self, action: int) -> Tuple[np.ndarray, float, bool, bool, Dict]:\n",
    "        \"\"\"Take an action and return next state, reward, done, truncated, info.\"\"\"\n",
    "        reward = 0\n",
    "        done = False\n",
    "        truncated = False\n",
    "\n",
    "        # Action 0: no pick, Action 1: pick formation top\n",
    "        if action == 1:\n",
    "            reward = self._calculate_pick_reward()\n",
    "            self.picked_tops.append(self.current_depth_idx)\n",
    "\n",
    "        # Move to next depth point\n",
    "        self.current_depth_idx += 1\n",
    "        self.step_count += 1\n",
    "\n",
    "        # Check if episode is done\n",
    "        if self.current_depth_idx >= len(self.well_data) or self.step_count >= self.max_steps:\n",
    "            done = True\n",
    "\n",
    "        self.episode_reward += reward\n",
    "\n",
    "        return self._get_observation(), reward, done, truncated, {\n",
    "            'picked_tops': self.picked_tops.copy(),\n",
    "            'total_reward': self.episode_reward\n",
    "        }\n",
    "\n",
    "    def _calculate_pick_reward(self) -> float:\n",
    "        \"\"\"Calculate reward for picking a formation top.\"\"\"\n",
    "        if self.current_depth_idx >= len(self.well_data):\n",
    "            return 0\n",
    "\n",
    "        current_data = self.well_data.iloc[self.current_depth_idx]\n",
    "        reward = 0\n",
    "\n",
    "        # 1. Lithology change reward\n",
    "        if self.current_depth_idx > 0:\n",
    "            prev_data = self.well_data.iloc[self.current_depth_idx - 1]\n",
    "            lithology_change = abs(current_data['VSHALE_NORM'] - prev_data['VSHALE_NORM'])\n",
    "            reward += self.reward_weights['lithology_change'] * lithology_change\n",
    "\n",
    "        # 2. Depth spacing reward (penalize too close picks)\n",
    "        if len(self.picked_tops) > 0:\n",
    "            min_distance = min(abs(self.current_depth_idx - top) for top in self.picked_tops)\n",
    "            if min_distance < 10:  # Too close\n",
    "                reward -= self.reward_weights['depth_spacing'] * (10 - min_distance)\n",
    "            else:\n",
    "                reward += self.reward_weights['depth_spacing'] * min(1, min_distance / 50)\n",
    "\n",
    "        # 3. Log response reward (reward for significant changes in multiple logs)\n",
    "        log_changes = []\n",
    "        for col in ['GR_NORM', 'NPHI_NORM', 'RHOB_NORM', 'RT_NORM']:\n",
    "            if self.current_depth_idx > 0:\n",
    "                prev_val = self.well_data.iloc[self.current_depth_idx - 1][col]\n",
    "                curr_val = current_data[col]\n",
    "                log_changes.append(abs(curr_val - prev_val))\n",
    "\n",
    "        avg_log_change = np.mean(log_changes) if log_changes else 0\n",
    "        reward += self.reward_weights['log_response'] * avg_log_change\n",
    "\n",
    "        # 4. Geological consistency reward\n",
    "        if len(self.picked_tops) > 1:\n",
    "            # Check if picks follow a reasonable pattern\n",
    "            depths = sorted(self.picked_tops)\n",
    "            intervals = [depths[i+1] - depths[i] for i in range(len(depths)-1)]\n",
    "            if intervals:\n",
    "                consistency = 1 / (1 + np.std(intervals) / np.mean(intervals))\n",
    "                reward += self.reward_weights['geological_consistency'] * consistency\n",
    "\n",
    "        return reward\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SMART AGENTS FOR FORMATION TOP PICKING\n",
    "# ============================================================================\n",
    "class HeuristicFormationAgent:\n",
    "    \"\"\"Heuristic agent that uses geological rules for formation top picking.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.name = \"Heuristic Formation Agent\"\n",
    "        self.last_pick_depth = -100  # Minimum spacing between picks\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        # Extract log features from observation\n",
    "        log_features = observation[:-4]  # Remove context features\n",
    "        num_logs = 6\n",
    "        window_size = len(log_features) // num_logs\n",
    "\n",
    "        # Get current depth point (middle of window)\n",
    "        current_idx = window_size // 2\n",
    "        if current_idx >= window_size:\n",
    "            return 0\n",
    "\n",
    "        # Extract current log values\n",
    "        gr_idx = current_idx\n",
    "        rt_idx = current_idx + 3 * window_size\n",
    "        vshale_idx = current_idx + 4 * window_size\n",
    "        phit_idx = current_idx + 5 * window_size\n",
    "\n",
    "        gr_val = log_features[gr_idx]\n",
    "        rt_val = log_features[rt_idx]\n",
    "        vshale_val = log_features[vshale_idx]\n",
    "        phit_val = log_features[phit_idx]\n",
    "\n",
    "        # Heuristic rules for formation top picking\n",
    "        pick_signals = 0\n",
    "\n",
    "        # Rule 1: Significant shale volume change\n",
    "        if current_idx > 0:\n",
    "            prev_vshale = log_features[current_idx - 1 + 4 * window_size]\n",
    "            if abs(vshale_val - prev_vshale) > 0.3:\n",
    "                pick_signals += 1\n",
    "\n",
    "        # Rule 2: Gamma ray spike (shale indicator)\n",
    "        if gr_val > 0.5:\n",
    "            pick_signals += 1\n",
    "\n",
    "        # Rule 3: Porosity change\n",
    "        if current_idx > 0:\n",
    "            prev_phit = log_features[current_idx - 1 + 5 * window_size]\n",
    "            if abs(phit_val - prev_phit) > 0.2:\n",
    "                pick_signals += 1\n",
    "\n",
    "        # Rule 4: Resistivity change\n",
    "        if current_idx > 0:\n",
    "            prev_rt = log_features[current_idx - 1 + 3 * window_size]\n",
    "            if abs(rt_val - prev_rt) > 0.4:\n",
    "                pick_signals += 1\n",
    "\n",
    "        # Decide to pick if enough signals\n",
    "        return 1 if pick_signals >= 2 else 0\n",
    "\n",
    "\n",
    "class QLearningFormationAgent:\n",
    "    \"\"\"Q-Learning agent for formation top picking.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size=100, action_size=2, learning_rate=0.1, epsilon=0.1, gamma=0.95):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = gamma\n",
    "        self.q_table = {}\n",
    "        self.name = \"Q-Learning Formation Agent\"\n",
    "\n",
    "    def _get_state_key(self, observation):\n",
    "        # Discretize continuous state for Q-table\n",
    "        # Use first 100 features and discretize them\n",
    "        features = observation[:100]\n",
    "        # Round to 1 decimal place for discretization\n",
    "        state = tuple(round(f, 1) for f in features)\n",
    "        return state\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        state = self._get_state_key(observation)\n",
    "\n",
    "        # Initialize Q-values for new state\n",
    "        if state not in self.q_table:\n",
    "            self.q_table[state] = np.zeros(self.action_size)\n",
    "\n",
    "        # Epsilon-greedy strategy\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0, self.action_size - 1)\n",
    "        else:\n",
    "            return np.argmax(self.q_table[state])\n",
    "\n",
    "    def learn(self, observation, action, reward, next_observation, done):\n",
    "        state = self._get_state_key(observation)\n",
    "        next_state = self._get_state_key(next_observation)\n",
    "\n",
    "        # Initialize Q-values if needed\n",
    "        if state not in self.q_table:\n",
    "            self.q_table[state] = np.zeros(self.action_size)\n",
    "        if next_state not in self.q_table:\n",
    "            self.q_table[next_state] = np.zeros(self.action_size)\n",
    "\n",
    "        # Q-learning update\n",
    "        current_q = self.q_table[state][action]\n",
    "        max_next_q = np.max(self.q_table[next_state])\n",
    "        new_q = current_q + self.learning_rate * (reward + self.gamma * max_next_q * (1 - done) - current_q)\n",
    "        self.q_table[state][action] = new_q\n",
    "\n",
    "\n",
    "class DQNFormationAgent:\n",
    "    \"\"\"Deep Q-Network agent for formation top picking.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size=100, action_size=2, learning_rate=0.001, gamma=0.95,\n",
    "                 epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.995):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.name = \"DQN Formation Agent\"\n",
    "\n",
    "        # Neural Network for formation top picking\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(state_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, action_size)\n",
    "        )\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        if random.random() <= self.epsilon:\n",
    "            return random.randint(0, self.action_size - 1)\n",
    "\n",
    "        state = torch.FloatTensor(observation[:self.state_size]).unsqueeze(0)\n",
    "        q_values = self.model(state)\n",
    "        return np.argmax(q_values.detach().numpy())\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def replay(self, batch_size=32):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "\n",
    "        batch = random.sample(self.memory, batch_size)\n",
    "        states = torch.FloatTensor([e[0][:self.state_size] for e in batch])\n",
    "        actions = torch.LongTensor([e[1] for e in batch])\n",
    "        rewards = torch.FloatTensor([e[2] for e in batch])\n",
    "        next_states = torch.FloatTensor([e[3][:self.state_size] for e in batch])\n",
    "        dones = torch.BoolTensor([e[4] for e in batch])\n",
    "\n",
    "        current_q_values = self.model(states).gather(1, actions.unsqueeze(1))\n",
    "        next_q_values = self.model(next_states).max(1)[0].detach()\n",
    "        target_q_values = rewards + (self.gamma * next_q_values * ~dones)\n",
    "\n",
    "        loss = F.mse_loss(current_q_values.squeeze(), target_q_values)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "\n",
    "class PolicyGradientFormationAgent:\n",
    "    \"\"\"Policy Gradient agent for formation top picking.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size=100, action_size=2, learning_rate=0.01):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.name = \"Policy Gradient Formation Agent\"\n",
    "\n",
    "        # Policy network for formation top picking\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(state_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, action_size),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "        self.episode_rewards = []\n",
    "        self.episode_actions = []\n",
    "        self.episode_states = []\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        state = torch.FloatTensor(observation[:self.state_size]).unsqueeze(0)\n",
    "        action_probs = self.model(state)\n",
    "        action = torch.multinomial(action_probs, 1).item()\n",
    "        return action\n",
    "\n",
    "    def store_transition(self, state, action, reward):\n",
    "        self.episode_states.append(state[:self.state_size])\n",
    "        self.episode_actions.append(action)\n",
    "        self.episode_rewards.append(reward)\n",
    "\n",
    "    def update_policy(self):\n",
    "        if len(self.episode_rewards) == 0:\n",
    "            return\n",
    "\n",
    "        # Calculate discounted rewards\n",
    "        discounted_rewards = []\n",
    "        R = 0\n",
    "        for r in reversed(self.episode_rewards):\n",
    "            R = r + 0.99 * R\n",
    "            discounted_rewards.insert(0, R)\n",
    "\n",
    "        # Normalize rewards\n",
    "        discounted_rewards = torch.FloatTensor(discounted_rewards)\n",
    "        discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-9)\n",
    "\n",
    "        # Calculate loss\n",
    "        states = torch.FloatTensor(self.episode_states)\n",
    "        actions = torch.LongTensor(self.episode_actions)\n",
    "        action_probs = self.model(states)\n",
    "        selected_action_probs = action_probs.gather(1, actions.unsqueeze(1)).squeeze()\n",
    "\n",
    "        loss = -(torch.log(selected_action_probs) * discounted_rewards).mean()\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Clear episode data\n",
    "        self.episode_rewards = []\n",
    "        self.episode_actions = []\n",
    "        self.episode_states = []\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# TRAINING AND EVALUATION FUNCTIONS\n",
    "# ============================================================================\n",
    "def train_formation_agent(agent, env, episodes=100):\n",
    "    \"\"\"Train an agent for formation top picking.\"\"\"\n",
    "    print(f\"Training {agent.name} for {episodes} episodes...\")\n",
    "\n",
    "    episode_rewards = []\n",
    "    episode_picks = []\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        observation, info = env.reset()\n",
    "        total_reward = 0\n",
    "        picks = []\n",
    "\n",
    "        while True:\n",
    "            # Choose action\n",
    "            action = agent.choose_action(observation)\n",
    "\n",
    "            # Take action\n",
    "            next_observation, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "            # Store experience for learning agents\n",
    "            if hasattr(agent, 'learn'):\n",
    "                agent.learn(observation, action, reward, next_observation, done)\n",
    "            elif hasattr(agent, 'remember'):\n",
    "                agent.remember(observation, action, reward, next_observation, done)\n",
    "            elif hasattr(agent, 'store_transition'):\n",
    "                agent.store_transition(observation, action, reward)\n",
    "\n",
    "            if action == 1:\n",
    "                picks.append(env.current_depth_idx - 1)\n",
    "\n",
    "            total_reward += reward\n",
    "            observation = next_observation\n",
    "\n",
    "            if done or truncated:\n",
    "                break\n",
    "\n",
    "        # Update policy for policy gradient agent\n",
    "        if hasattr(agent, 'update_policy'):\n",
    "            agent.update_policy()\n",
    "\n",
    "        # Replay for DQN agent\n",
    "        if hasattr(agent, 'replay'):\n",
    "            agent.replay()\n",
    "\n",
    "        episode_rewards.append(total_reward)\n",
    "        episode_picks.append(len(picks))\n",
    "\n",
    "        if episode % 10 == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-10:])\n",
    "            avg_picks = np.mean(episode_picks[-10:])\n",
    "            print(f\"Episode {episode}, Avg Reward: {avg_reward:.2f}, Avg Picks: {avg_picks:.1f}\")\n",
    "\n",
    "    return episode_rewards, episode_picks\n",
    "\n",
    "\n",
    "def evaluate_formation_agent(agent, env, num_episodes=10):\n",
    "    \"\"\"Evaluate a trained agent.\"\"\"\n",
    "    print(f\"Evaluating {agent.name}...\")\n",
    "\n",
    "    all_rewards = []\n",
    "    all_picks = []\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        observation, info = env.reset()\n",
    "        total_reward = 0\n",
    "        picks = []\n",
    "\n",
    "        while True:\n",
    "            action = agent.choose_action(observation)\n",
    "            next_observation, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "            if action == 1:\n",
    "                picks.append(env.current_depth_idx - 1)\n",
    "\n",
    "            total_reward += reward\n",
    "            observation = next_observation\n",
    "\n",
    "            if done or truncated:\n",
    "                break\n",
    "\n",
    "        all_rewards.append(total_reward)\n",
    "        all_picks.append(picks)\n",
    "\n",
    "    avg_reward = np.mean(all_rewards)\n",
    "    avg_num_picks = np.mean([len(picks) for picks in all_picks])\n",
    "\n",
    "    print(f\"Average Reward: {avg_reward:.2f}\")\n",
    "    print(f\"Average Number of Picks: {avg_num_picks:.1f}\")\n",
    "\n",
    "    return all_rewards, all_picks\n",
    "\n",
    "\n",
    "def plot_formation_picks(well_data, picks, agent_name):\n",
    "    \"\"\"Plot well log data with picked formation tops.\"\"\"\n",
    "    _, axes = plt.subplots(1, 4, figsize=(15, 8))\n",
    "\n",
    "    # Plot each log\n",
    "    logs = ['GR', 'NPHI', 'RHOB', 'RT']\n",
    "    colors = ['green', 'blue', 'red', 'orange']\n",
    "\n",
    "    for i, (log, color) in enumerate(zip(logs, colors)):\n",
    "        axes[i].plot(well_data[log], well_data['DEPTH'], color=color, linewidth=1)\n",
    "        axes[i].set_title(f'{log} Log')\n",
    "        axes[i].set_ylabel('Depth')\n",
    "        axes[i].set_xlabel(log)\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "        axes[i].invert_yaxis()\n",
    "\n",
    "        # Mark picked formation tops\n",
    "        for pick_idx in picks:\n",
    "            if pick_idx < len(well_data):\n",
    "                pick_depth = well_data.iloc[pick_idx]['DEPTH']\n",
    "                axes[i].axhline(y=pick_depth, color='red', linestyle='--', alpha=0.7)\n",
    "\n",
    "    plt.suptitle(f'Formation Tops Picked by {agent_name}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffbed5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.getcwd())\n",
    "sys.path.append('..')\n",
    "import quick_pp.las_handler as las\n",
    "\n",
    "# Create synthetic well data\n",
    "file_name = r'C:\\projects\\quick_pp\\notebooks\\data\\01_raw\\36_7-3.las'\n",
    "with open(file_name, 'rb') as f:\n",
    "    df, _ = las.read_las_file_welly(f)\n",
    "well_data = df[['DEPTH', 'GR']]\n",
    "\n",
    "# Create environment\n",
    "env = FormationTopPickingEnv(well_data, max_depth_window=150)\n",
    "\n",
    "# Choose which agent to use\n",
    "agent_type = \"dqn\"  # Options: \"heuristic\", \"qlearning\", \"dqn\", \"policy_gradient\"\n",
    "\n",
    "if agent_type == \"heuristic\":\n",
    "    agent = HeuristicFormationAgent()\n",
    "    print(\"Using Heuristic Formation Agent\")\n",
    "elif agent_type == \"qlearning\":\n",
    "    agent = QLearningFormationAgent()\n",
    "    print(\"Using Q-Learning Formation Agent\")\n",
    "elif agent_type == \"dqn\":\n",
    "    agent = DQNFormationAgent()\n",
    "    print(\"Using DQN Formation Agent\")\n",
    "elif agent_type == \"policy_gradient\":\n",
    "    agent = PolicyGradientFormationAgent()\n",
    "    print(\"Using Policy Gradient Formation Agent\")\n",
    "else:\n",
    "    print(\"Invalid agent type, using heuristic\")\n",
    "    agent = HeuristicFormationAgent()\n",
    "\n",
    "# Train the agent\n",
    "if agent_type in [\"qlearning\", \"dqn\", \"policy_gradient\"]:\n",
    "    rewards, picks = train_formation_agent(agent, env, episodes=50)\n",
    "    print(f\"Training completed. Final average reward: {np.mean(rewards[-10:]):.2f}\")\n",
    "\n",
    "# Evaluate the agent\n",
    "eval_rewards, eval_picks = evaluate_formation_agent(agent, env, num_episodes=5)\n",
    "\n",
    "# Plot results\n",
    "if eval_picks:\n",
    "    plot_formation_picks(well_data, eval_picks[0], agent.name)\n",
    "\n",
    "print(\"Formation top picking RL demonstration completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
